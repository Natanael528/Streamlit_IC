{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33C3HvMknMvX",
   "metadata": {
    "id": "33C3HvMknMvX"
   },
   "source": [
    "- Dados: Focos de calor do INPE na frequência anual - https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/EstadosBr_sat_ref/MG/\n",
    "\n",
    "- Dados: Focos de calor do INPE na frequência mensal - https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/mensal/Brasil/\n",
    "\n",
    "- Código realizado por: Enrique V. Mattos - 11/06/2024 & Modificado por: Natanael Silva Oliveira - 16/12/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x-B_vISQlkTw",
   "metadata": {
    "id": "x-B_vISQlkTw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **1° Passo:** Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb585815",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8273,
     "status": "ok",
     "timestamp": 1716321757543,
     "user": {
      "displayName": "Enrique Vieira Mattos",
      "userId": "10885979489108607956"
     },
     "user_tz": 180
    },
    "id": "bb585815",
    "outputId": "2f271c0d-03f2-4316-9b05-13c75d7af6ca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os                        \n",
    "import requests                  \n",
    "from bs4 import BeautifulSoup   \n",
    "import re                       \n",
    "import glob                      \n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "\n",
    "# vamos ignorar vários avisos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7sGRIbDEZa5",
   "metadata": {
    "id": "S7sGRIbDEZa5"
   },
   "source": [
    "# **PARTE 1):** Baixando e organizando os dados de focos de calor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cz2kvfh1mmXE",
   "metadata": {
    "id": "cz2kvfh1mmXE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## - Baixando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RkaftoMUoE2m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52461,
     "status": "ok",
     "timestamp": 1716321857578,
     "user": {
      "displayName": "Enrique Vieira Mattos",
      "userId": "10885979489108607956"
     },
     "user_tz": 180
    },
    "id": "RkaftoMUoE2m",
    "outputId": "489dcdab-cf81-4dd9-a5f5-1122f7f9d127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando e processando: focos_br_ref_2003.zip\n",
      "Baixando e processando: focos_br_ref_2004.zip\n",
      "Baixando e processando: focos_br_ref_2005.zip\n",
      "Baixando e processando: focos_br_ref_2006.zip\n",
      "Baixando e processando: focos_br_ref_2007.zip\n",
      "Baixando e processando: focos_br_ref_2008.zip\n",
      "Baixando e processando: focos_br_ref_2009.zip\n",
      "Baixando e processando: focos_br_ref_2010.zip\n",
      "Baixando e processando: focos_br_ref_2011.zip\n",
      "Baixando e processando: focos_br_ref_2012.zip\n",
      "Baixando e processando: focos_br_ref_2013.zip\n",
      "Baixando e processando: focos_br_ref_2014.zip\n",
      "Baixando e processando: focos_br_ref_2015.zip\n",
      "Baixando e processando: focos_br_ref_2016.zip\n",
      "Baixando e processando: focos_br_ref_2017.zip\n",
      "Baixando e processando: focos_br_ref_2018.zip\n",
      "Baixando e processando: focos_br_ref_2019.zip\n",
      "Baixando e processando: focos_br_ref_2020.zip\n",
      "Baixando e processando: focos_br_ref_2021.zip\n",
      "Baixando e processando: focos_br_ref_2022.zip\n",
      "Baixando e processando: focos_br_ref_2023.zip\n",
      "Baixando e processando: focos_mensal_br_202401.csv\n",
      "Baixando e processando: focos_mensal_br_202402.csv\n",
      "Baixando e processando: focos_mensal_br_202403.csv\n",
      "Baixando e processando: focos_mensal_br_202404.csv\n",
      "Baixando e processando: focos_mensal_br_202405.csv\n",
      "Baixando e processando: focos_mensal_br_202406.csv\n",
      "Baixando e processando: focos_mensal_br_202407.csv\n",
      "Baixando e processando: focos_mensal_br_202408.csv\n",
      "Baixando e processando: focos_mensal_br_202409.csv\n",
      "Baixando e processando: focos_mensal_br_202410.csv\n",
      "Baixando e processando: focos_mensal_br_202411.csv\n",
      "Baixando e processando: focos_mensal_br_202412.csv\n",
      "Dados de 2003 a 2023:\n",
      "                  data     lat     lon                 municipio  \\\n",
      "0  2003-05-15 17:05:00 -18.518 -55.028  RIO VERDE DE MATO GROSSO   \n",
      "1  2003-05-15 17:05:00 -17.542 -48.815               PIRACANJUBA   \n",
      "2  2003-05-15 17:05:00 -17.612 -47.160                GUARDA-MOR   \n",
      "3  2003-05-15 17:05:00 -18.386 -51.919              SERRANÓPOLIS   \n",
      "4  2003-05-15 17:05:00 -17.877 -49.098                 MORRINHOS   \n",
      "\n",
      "               estado    bioma  \n",
      "0  MATO GROSSO DO SUL  Cerrado  \n",
      "1               GOIÁS  Cerrado  \n",
      "2        MINAS GERAIS  Cerrado  \n",
      "3               GOIÁS  Cerrado  \n",
      "4               GOIÁS  Cerrado  \n",
      "\n",
      "Dados de 2024:\n",
      "                                     id      lat      lon  \\\n",
      "0  966ed445-fa70-3369-9802-3e74c5765684 -12.5667 -41.4364   \n",
      "1  0669c14d-7a39-3d98-9f20-23e118fbcf09 -12.5662 -41.4611   \n",
      "2  53a46800-fa40-3323-9b20-05108d435c24 -18.0629 -57.3721   \n",
      "3  d0af560b-c440-3554-a5c6-83fb055304e8 -18.0823 -57.3902   \n",
      "4  b509b339-729d-3a30-a7d9-8a6a33398a4f -18.0823 -57.3902   \n",
      "\n",
      "         data_hora_gmt satelite municipio              estado    pais  \\\n",
      "0  2024-01-01 00:06:16  GOES-16   LENÇÓIS               BAHIA  Brasil   \n",
      "1  2024-01-01 00:06:16  GOES-16   LENÇÓIS               BAHIA  Brasil   \n",
      "2  2024-01-01 00:06:48  GOES-16   CORUMBÁ  MATO GROSSO DO SUL  Brasil   \n",
      "3  2024-01-01 00:06:48  GOES-16   CORUMBÁ  MATO GROSSO DO SUL  Brasil   \n",
      "4  2024-01-01 00:16:48  GOES-16   CORUMBÁ  MATO GROSSO DO SUL  Brasil   \n",
      "\n",
      "   municipio_id  estado_id  pais_id  numero_dias_sem_chuva  precipitacao  \\\n",
      "0       2919306         29       33                    8.0          1.38   \n",
      "1       2919306         29       33                    9.0          1.73   \n",
      "2       5003207         50       33                    0.0         14.42   \n",
      "3       5003207         50       33                    0.0         12.93   \n",
      "4       5003207         50       33                    0.0         12.93   \n",
      "\n",
      "   risco_fogo     bioma   frp  \n",
      "0        0.70  Caatinga  76.5  \n",
      "1        0.83  Caatinga  81.4  \n",
      "2        0.00  Pantanal  59.8  \n",
      "3     -999.00  Pantanal  63.4  \n",
      "4     -999.00  Pantanal  64.6  \n"
     ]
    }
   ],
   "source": [
    "# URLs base\n",
    "url_ano_anteriores = 'https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/'\n",
    "url_ano_atual = 'https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/mensal/Brasil/'\n",
    "\n",
    "#================================================================#\n",
    "#           ANOS ANTERIORES: focos_br_ref_2003.zip    \n",
    "#================================================================#\n",
    "# Faz a requisição para pegar os links dos arquivos ZIP\n",
    "result = requests.get(url_ano_anteriores)\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "zip_files = soup.find_all('a', href=re.compile(\"\\.zip\"))\n",
    "\n",
    "# Lista para armazenar DataFrames de 2003 a 2023\n",
    "dataframes_2003_a_2023 = []\n",
    "\n",
    "for i in zip_files:\n",
    "    filename = i.get_text().strip()\n",
    "    url = f'{url_ano_anteriores}{filename}'\n",
    "    print(f\"Baixando e processando: {filename}\")\n",
    "\n",
    "    # Faz o download do arquivo ZIP\n",
    "    response = requests.get(url)\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "        # Extrai e lê o arquivo CSV dentro do ZIP\n",
    "        for file in z.namelist():\n",
    "            with z.open(file) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                \n",
    "                # Renomeia colunas para compatibilidade no arquivo de 2023\n",
    "                if '2023' in filename:\n",
    "                    df.rename(columns={'latitude': 'lat', 'longitude': 'lon'}, inplace=True)\n",
    "                \n",
    "                dataframes_2003_a_2023.append(df)\n",
    "\n",
    "# Concatena todos os DataFrames em um só\n",
    "df_2003_a_2023 = pd.concat(dataframes_2003_a_2023, ignore_index=True)\n",
    "\n",
    "# Remove colunas desnecessárias e ajusta os nomes\n",
    "df_2003_a_2023.drop(['id_bdq', 'foco_id', 'pais'], axis=1, inplace=True)\n",
    "df_2003_a_2023.rename(columns={'data_pas': 'data'}, inplace=True)\n",
    "\n",
    "# Reorganiza as colunas\n",
    "df_2003_a_2023 = df_2003_a_2023[['data', 'lat', 'lon', 'municipio', 'estado', 'bioma']]\n",
    "\n",
    "#================================================================#\n",
    "#            ANO ATUAL: focos_mensal_br_202401.csv\t\n",
    "#================================================================#\n",
    "result = requests.get(url_ano_atual)\n",
    "soup = BeautifulSoup(result.content, 'html.parser')\n",
    "csv_files = soup.find_all('a', href=re.compile(\"\\.csv\"))\n",
    "\n",
    "# Lista para armazenar DataFrames do ano atual\n",
    "dataframes_2024 = []\n",
    "\n",
    "for i in csv_files:\n",
    "    filename = i.get_text().strip()\n",
    "    url = f'{url_ano_atual}{filename}'\n",
    "    print(f\"Baixando e processando: {filename}\")\n",
    "\n",
    "    # Faz o download e lê o arquivo CSV\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_csv(BytesIO(response.content))\n",
    "    dataframes_2024.append(df)\n",
    "\n",
    "# Concatena os DataFrames do ano atual\n",
    "df_2024 = pd.concat(dataframes_2024, ignore_index=True)\n",
    "\n",
    "# seleciona para o satélite de referência AQUA_M-T\n",
    "df_2024 = df_2024[df_2024['satelite']=='AQUA_M-T']\n",
    "\n",
    "# remove colunas\n",
    "df_2024.drop(['satelite'], axis=1, inplace=True)\n",
    "\n",
    "# renomeia coluna\n",
    "df_2024.rename(columns={'data_hora_gmt': 'data'}, inplace=True)\n",
    "\n",
    "# reposiciona as colunas\n",
    "df_2024 = df_2024[['data','lat','lon','municipio','estado','bioma']]\n",
    "\n",
    "#================================================================#\n",
    "# Mostrando os DataFrames gerados\n",
    "#================================================================#\n",
    "\n",
    "df_2003_a_2023\n",
    "\n",
    "df_2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b_s7_9ciLdbc",
   "metadata": {
    "id": "b_s7_9ciLdbc"
   },
   "source": [
    "## - Junta e salva os dados de `2003-2023` com `2024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "BZ3lIfhVLTV0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1716322367928,
     "user": {
      "displayName": "Enrique Vieira Mattos",
      "userId": "10885979489108607956"
     },
     "user_tz": 180
    },
    "id": "BZ3lIfhVLTV0",
    "outputId": "6b6ab619-4d76-4dde-99ce-ea0d5465beea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 21.1 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# junta os dataframes dos anos anteriores com o ano atual\n",
    "df_total = pd.concat([df_2003_a_2023,df_2024], ignore_index=True)\n",
    "\n",
    "#=================================================#\n",
    "#                TODAS COLUNAS\n",
    "#=================================================#\n",
    "dfx = df_total.copy()\n",
    "\n",
    "#=================================================#\n",
    "#                   ESTADOS\n",
    "#=================================================#\n",
    "dfx = df_total.copy() ; dfx.drop(['data','lat','lon','municipio','bioma'], axis=1, inplace=True)\n",
    "dfx.to_csv('estados.csv', compression={'method': 'zip'}, index=False)\n",
    "\n",
    "#=================================================#\n",
    "#                   MUNÍCIPIOS\n",
    "#=================================================#\n",
    "dfx = df_total.copy() ; dfx.drop(['data','lat','lon','estado','bioma'], axis=1, inplace=True)\n",
    "dfx.to_csv('municipios.csv', compression={'method': 'zip'}, index=False)\n",
    "\n",
    "#=================================================#\n",
    "#                 LATITUDE\n",
    "#=================================================#\n",
    "dfx = df_total.copy() \n",
    "\n",
    "# multiplica por 10.000 e transforma de real para inteiro para inteiro. Diminuir o tamanho da variável.\n",
    "dfx['lat'] = dfx['lat']*10000.\n",
    "dfx['lat'] = dfx['lat'].astype(np.int32)\n",
    "\n",
    "# salva CSV\n",
    "dfx.drop(['lon','estado','municipio','bioma'], axis=1, inplace=True)\n",
    "dfx.to_csv('lat.csv', compression={'method': 'zip'}, index=False)\n",
    "\n",
    "#=================================================#\n",
    "#                 LONGITUDE \n",
    "#=================================================#\n",
    "dfx = df_total.copy() \n",
    "# multiplica por 10.000 e transforma de real para inteiro. Diminuir o tamanho da variável.\n",
    "dfx['lon'] = dfx['lon']*10000.\n",
    "dfx['lon'] = dfx['lon'].astype(np.int32)\n",
    "\n",
    "# salva CSV\n",
    "dfx.drop(['data','lat','estado','municipio','bioma'], axis=1, inplace=True)\n",
    "dfx.to_csv('lon.csv', compression={'method': 'zip'}, index=False)\n",
    "\n",
    "#=================================================#\n",
    "#                  BIOMA\n",
    "#=================================================#\n",
    "dfx = df_total.copy() \n",
    "dfx.drop(['data','lat','lon','estado','municipio'], axis=1, inplace=True)\n",
    "dfx.to_csv('biomas.csv', compression={'method': 'zip'}, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0jFqWV9Dliao",
    "x-B_vISQlkTw",
    "V2X0HzSXwuSg",
    "EnO1rSEVwynL",
    "S7sGRIbDEZa5",
    "cz2kvfh1mmXE",
    "NjvhQGf9qOJw",
    "Yh7F1AsL5c1k",
    "b_s7_9ciLdbc",
    "nu_aOiKmElsz",
    "eTzL-HZrEqzB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
